{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 总结"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. SparkSession 创建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL basic example\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. DataFrame 创建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark is an existing SparkSession\n",
    "df = spark.read.json(\"examples/src/main/resources/people.json\")\n",
    "# Displays the content of the DataFrame to stdout\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. DataFrame 操作\n",
    "##### 3.1 df.printSchema()\n",
    "##### 3.2 df.select('columns').show()\n",
    "##### 3.3 df.select(df['xx'], df['xxx']+1).show()\n",
    "##### 3.4 df.filter(df['xx']>='').show()\n",
    "##### 3.5 df.groupBy('xxx').count().show()\n",
    "##### 3.6 df.show()\n",
    "##### 3.7 df.show(n), top n."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. RDD创建\n",
    "##### 4.1 sc.tesxtFile('people.txt')\n",
    "##### 4.2 sc.parallelize('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. RDD 转 DataFrame\n",
    "##### 5.1 Inferring the Schema Using Reflection 根据返回推断\n",
    "##### 5.2 Programmatically Specifying the Schema 预先指定"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. 聚合Aggregation\n",
    "##### 6.1 count()\n",
    "##### 6.2 countDistinct()\n",
    "##### 6.3 avg()\n",
    "##### 6.4 max()\n",
    "##### 6.5 min()\n",
    "##### 6.6 etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. DataFrame操作转Sql查询格式\n",
    "##### 7.1 createOrReplaceTempView(临时表，Session结束终止)\n",
    "##### 7.2 createGlobalTempView(全局表,Application结束终止)\n",
    "##### 7.3 跨session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.1 \n",
    "df.createOrReplaceTempView(\"people\")\n",
    "sqlDF = spark.sql(\"SELECT * FROM people\")\n",
    "sqlDF.show()\n",
    "\n",
    "# 7.2 \n",
    "df.createGlobalTempView(\"people\")\n",
    "spark.sql(\"SELECT * FROM global_temp.people\").show()\n",
    "\n",
    "# 7.3 跨Session\n",
    "# Global temporary view is cross-session\n",
    "spark.newSession().sql(\"SELECT * FROM global_temp.people\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
