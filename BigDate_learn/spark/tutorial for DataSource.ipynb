{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL data source example\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 通用的导入保存函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.load('people.json', format='json')\n",
    "print(type(df))\n",
    "# df.select(\"name\", \"favorite_color\").write.save(\"namesAndFavoColors.parquet\", mode='overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. 人工指定参数\n",
    "##### json, parquet, jdbc, orc, libsvm, csv, text都可以"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 其中 format 可选.\n",
    "# load json file\n",
    "df_json = spark.read.load(\"examples/src/main/resources/people.json\", format=\"json\")\n",
    "\n",
    "# 其中 format 可选, \n",
    "# mode 可选 ['error'(有就报错), 'append'(追加), 'overwrite'(覆盖), 'ignore'(忽略)]\n",
    "# load parquet file\n",
    "df_parquet.select(\"name\", \"age\").write.save(\"namesAndAges.parquet\", format=\"parquet\", mode='ignore')\n",
    "\n",
    "# load csv file\n",
    "df_csv = spark.read.load(\"examples/src/main/resources/people.csv\",\\\n",
    "                     format=\"csv\", sep=\":\", inferSchema=\"true\", header=\"true\")\n",
    "\n",
    "# load orc fils\n",
    "df_orc = spark.read.orc(\"examples/src/main/resources/users.orc\")\n",
    "(df_orc.write.format(\"orc\")\\\n",
    "       .option(\"orc.bloom.filter.columns\", \"favorite_color\")\\\n",
    "       .option(\"orc.dictionary.key.threshold\", \"1.0\")\\\n",
    "       .save(\"users_with_options.orc\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Run SQL on files directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "df_ = spark.sql(\"SELECT * FROM parquet.`namesAndFavoColors.parquet`\")\n",
    "print(type(df_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. 持久化到Tables\n",
    "##### 持久化到Hive metastore\n",
    "##### 即使Session重新启动, 只要保持connection不变, 表仍然存在.\n",
    "##### 持久化的表, 可以用SQL方法.\n",
    "##### 可以指定路径, 把text/parquet/json等文件持久化到表. df.write.option('path', '/some/path').saveAsTable('t').\n",
    "##### 即使用户表被删除了, 数据仍然在."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Bucketing, Sorting and Partitioning\n",
    "##### Bucketing, Sorting仅适用于Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.bucketBy(42, \"name\").sortBy(\"name\").saveAsTable(\"people_bucketed\", mode='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_.write.partitionBy(\"favorite_color\").format(\"parquet\").save(\"namesPartByColor.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet(\"examples/src/main/resources/users.parquet\")\n",
    "(df.write.partitionBy(\"favorite_color\")\\\n",
    "         .bucketBy(42, \"name\")\\\n",
    "         .saveAsTable(\"people_partitioned_bucketed\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------+\n",
      "|  name|favorite_color|\n",
      "+------+--------------+\n",
      "|Alyssa|          null|\n",
      "|   Ben|           red|\n",
      "+------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM parquet.`namesPartByColor.parquet`\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"DROP TABLE IF EXISTS people_bucketed\")\n",
    "# spark.sql(\"DROP TABLE IF EXISTS people_bucketed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------+\n",
      "|  name|favorite_color|\n",
      "+------+--------------+\n",
      "|Alyssa|          null|\n",
      "|   Ben|           red|\n",
      "+------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
